{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e7e08f-dd64-4052-b9c8-e264668243ce",
   "metadata": {},
   "source": [
    "<h1><center>ğ“¡ğ“®ğ“¬ğ“¾ğ“»ğ“»ğ“®ğ“·ğ“½ ğ“·ğ“®ğ“¾ğ“»ğ“ªğ“µ ğ“·ğ“®ğ“½ğ”€ğ“¸ğ“»ğ“´</center></h1>\n",
    "\n",
    "[**Please read this before you start**](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d08530-f5f5-4ca6-90e2-ba2dc55ace44",
   "metadata": {},
   "source": [
    "**Why sequence of characters is usefull for rnn why not other models?** (why can't we use normal neural network for this kind of task)\n",
    "\n",
    "The answer is the input and output vectors will mis-match with each other and the neural networks does not transfer the information to the next neural network, in our case(Nlp) needs sharing parameters, this are the two most bad dis-advantage of the normal neural network when we use the sequence of models. but in rnn there is no dis-advantage what we seen earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d63b7-1ec3-4df6-8cbd-a766c5b1270f",
   "metadata": {},
   "source": [
    "Humans donâ€™t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You donâ€™t throw everything away and start thinking from scratch again. Your thoughts have persistence.\n",
    "\n",
    "Traditional neural networks canâ€™t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. Itâ€™s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\n",
    "\n",
    "Sequence models such as rnn, lstm, gru are popular models for sequence inputs(like sequence of word, or numbers or images). Ther are different types of sequence tasks available like speech recogonition(x), output is text of the speech(y), and sentiment classificaiton input is (sequence of text), output is classification(pos, or neg) and more. \n",
    "\n",
    "\n",
    "<img src=\"images/examples_sequence.jpg\" width='600'/>\n",
    "\n",
    "<img src=\"images/sequence.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51fa39-0d19-4422-9003-df629dc9da20",
   "metadata": {},
   "source": [
    "Let's go in detial about rnn, consider we need to give a sequence of words to rnn, words can't understand by the machines. So, we need to represent the words into numbers, there are lot's of ways is here but one of the easy wasy is one hot representation. Just simple as the heading. \n",
    "\n",
    "\n",
    "<img src=\"images/one_hot.png\" width=\"500\"/>   <img src=\"images/one_hot_gif.gif\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d82fb-ced0-4746-ae5d-80d3c6007c8a",
   "metadata": {},
   "source": [
    "**Let's deep dive to rnn**, our task is to find the POS of the given sentence,we encoded our sentence to the one hot vector. When you are giving the first one hot vector x(1) to our rnn(have some hidden layers) it will predict the output yhat(1). But not only predicting it will share the activation information a(1) to the next network with the time step T. Then this process continues until total number of vectors (Tx) we have. \n",
    "\n",
    "<img src=\"images/rnn.PNG\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315374d-5876-4640-ae89-521571701b8b",
   "metadata": {},
   "source": [
    "And another one main thing, the activation value is also needed for first input layer of the vector, so scientist pass just the 0 encoded vector(dummy), they create a dummy vector for first input(activation) of the layer. The rnn usally flows from **Left to Right**. The parameters(weights) from first layers will be travelling until last hidden layer of the rnn architecture, that's how rnn are maintaing the sequence of the information. **In simple words same weights for all the rnn architecture**. The only learnable parameter is the input weights only.  \n",
    "\n",
    "<img src=\"images/sharing.gif\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887b161-c82b-4a2d-b2c6-bd0ba33cc084",
   "metadata": {},
   "source": [
    "The problem in the uni-directional rnn, it will predict based on earlier prediction(left to right) but in our case(nlp) we need to look on all the sentence for exact information retrivel, so this is the limitation of the rnn, it can be overtake by bi-directional rnn(look on later). \n",
    "\n",
    "<img src=\"images/problem.png\" width=\"500\"/>   <img src=\"images/problem1..png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c528a9f-b820-42e8-85cc-451d76e08bf5",
   "metadata": {},
   "source": [
    "### Forward prop\n",
    "\n",
    "<img src='images/forward_rnn.gif' width='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc31fde-1c11-48c1-b6e6-7497af8ef7a6",
   "metadata": {},
   "source": [
    "<img src=\"images/form.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a31e1-87f4-499b-b25d-1e06739496ba",
   "metadata": {},
   "source": [
    "Wax -- Weight matrix multiplying the input,\n",
    "\n",
    "Waa -- Weight matrix multiplying the hidden state,\n",
    "                        \n",
    "Wya -- Weight matrix relating the hidden-state to the output,\n",
    "                        \n",
    "ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        \n",
    "by -- Bias relating the hidden-state to the output, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f2074-c785-4fe7-8921-b705db6a6259",
   "metadata": {},
   "source": [
    "## Backward Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be5dbc-7d93-4444-8de7-49183f557a32",
   "metadata": {},
   "source": [
    "**Review of a forward propagation**. \n",
    "\n",
    "<img src=\"images/review_forward.png\" width=\"600\"/>   <img src=\"images/back.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80a840-377d-4f23-989f-fb358771dba1",
   "metadata": {},
   "source": [
    "Still now, we have discussed about only the fixed type of input and fixed type of output(I mean input is equal to output) but in many cases the input and output are not equal and they will have lot's and lots' variation, let's see the variation of rnn alongside\n",
    "\n",
    "\n",
    "<img src=\"images/diff.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d76d4a-6323-4ce3-83d9-494ea2802f71",
   "metadata": {},
   "source": [
    "**(1) One to One** -> image classification \n",
    "\n",
    "**(2) One to Many** -> image summarisation \n",
    "\n",
    "**(3) Many to One** -> sentiment classification \n",
    "\n",
    "**(4) Many to Many non fixed** -> Machine Translation \n",
    "\n",
    "**(5) Many to Many fixed** -> POS tagging \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdd4ab-1eaa-42db-b7c8-7cf578ed34f5",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Descent\n",
    "\n",
    "The basic problem of the rnn is that it runs into **vanishing gradient problem**, Why vanishing gradient, because our normal rnn, just capture the earlier words(for example: \" I am having a cat\"-> rnn will predict based on earlier words(a, cat) and it does not have influence of word in the beginning)) This is the problem of rnn, when you have exploding gradient you can easily findout by seeing the weights but in our case vanishing is difficult to find out it never convergence. So tackle this we will use some other models. like models (**GRU** and **LSTM**)\n",
    "\n",
    "\n",
    "<img src=\"images/vanish.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37b862-416a-4149-b04b-76e44cc10b87",
   "metadata": {},
   "source": [
    "The normal rnn has a vanishing gradient problem instead of that the lstm, gru is also having a another main problem is that, they will see the past words only in nlp it's not possilbe to predict by past, we need to use the future sentences also to predict properly. \n",
    "\n",
    "ex: He said \"Teddy bears are on sale\" \n",
    "ex: He said \"Teddy **Rooselvelt** was a great president\". -> to capture he is a person not teddy we need to use the future also, for that we will use the bidirectional rnn\n",
    "\n",
    "\n",
    "## Bidirectional RNN\n",
    "It's nothing but during forward propagation the process (left to right) and the same time another side (right to side) processing will happen, this will helps to find the future present and the past also. \n",
    "\n",
    "<img src=\"images/bro.gif\" width=\"600\"/>  <img src=\"images/kl.png\" width=\"600\"/>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8146d1-5bb1-45c8-9e1f-2154a190c9bc",
   "metadata": {},
   "source": [
    "The main dis-advantage is that, when you using this for speech recogonition you should wait for the person stoping the speech, because this works when we have full set of sentences, for some other particular tasks this will be better. The main thing is that we need to have a complete set of text to use this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27900f43-3d47-4524-b0d1-884d6e314032",
   "metadata": {},
   "source": [
    "## Deep RNN \n",
    "\n",
    "Still now we have seen lstm, rnn, and gru but if you want to learn more complex relationship between the data you need to use the more complex models, (I mean you need to stack up more rnn and lstm up to gether to learn the patterns in our data). \n",
    "\n",
    "Just stacking more rnn and lstm vertically or horizontly to get pattern of the data, This is called deep rnn. deep recurrent network layer.\n",
    "\n",
    "\n",
    "<img src=\"images/deep.png\" width=\"800\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4816d-5b2c-473c-b71b-87715583d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"images/RNN.PNG\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
